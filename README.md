# esgf_wms

This repo contains scripts related to modifying ESGF-produced THREDDS catalogs
and creating NcML aggregations.

Install requirements with `pip install -r requirements.txt` (create and
activate a python3 virtualenv first).

## Publication

### modify_catalogs.py

This script modifies THREDDS xml catalogs generated by the ESGF publisher to
remove ESGF-specific markup, and optionally creates NcML aggregations and makes
these accessible through OPeNDAP/WMS/WCS.

Basic usage: `./modify_catalogs.py <catalog> [<catalog> ...]`

This will process the catalog(s) given and write the modified catalog(s) to a
file in `output_catalogs` with the same basename as `<catalog>`.

If `--aggregate` is used, NcML files for the aggregate datasets will be saved
in `aggregations` and OPeNDAP endpoints are added. Use `--wms` to additionally
create WMS/WCS endpoints.

(The directory names `input_catalogs`, `output_catalogs` and `aggregations` can
be overridden with the `--input-dir`, `--output-dir` and `--ncml-dir` options
respectively)

The code assumes that the files and directories created in `aggregations` will
be placed under `/usr/local/aggregations` on the live server.

### make_mapfiles.py

This script generates ESGF mapfiles from a JSON file of the form

```json
{
    "<unversioned_dataset_name>": {
        "generate_aggregation": <boolean>,
        "include_in_wms": <boolean>,
        "tech_note_url": "<url>",
        "tech_note_title": "<title>",
        "files": [
            {
                "path": "<path>",
                "sha256": "<checksum>",
                "mtime": "<mtime>",
                "size": "<size in bytes>"
            },
            ...
        ]
    },
    ...
}
```

See `./make_mapfiles.py --help` for more info.

### merge_csv_json.py

This script is located in `publication_utils`.

Usage:  `./merge_csv_json.py <input CSV>`.

Read a CSV file containing information about datasets to be published and
print JSON in the format required by `make_mapfiles.py` to stdout.

See `./merge_csv_json.py --help` for the required format of the CSV.

## Aggregation helper scripts

These scripts are located in `aggregation_utils`.

### ./partition_files.py

Usage: `./partition_files.py <outdir>`.

Read file paths from standard input and partitions them into groups of files that can likely be
aggregated based on their filenames. These groups are written to files `<outdir>/1`, `<outdir>/2`
etc...

### ./aggregate.py

Read file paths from standard input (one per line) and write an NcML aggregation of those files to
standard output.

### ./agg_wrapper.sh

A convenience script `./agg_wrapper.sh <dir>` finds NetCDF files in `<dir>`, runs
`partition_files.py` on the list and `aggregate.py` on each output.

## Tests

`tests.py` contains some *very simple* tests - to run:

* Create a directory `test_input_catalogs` containing an un-modified THREDDS
  catalog (this is used as a base catalog to modify during the tests - but the
  modified catalog will *not* be written to disk)
* Run `pytest tests.py`.

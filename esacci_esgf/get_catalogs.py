#!/usr/bin/env python3
"""
Get dataset names from JSON file(s), find the associated THREDDS catalogs, and
run `modify_catalogs.py' on each with appropriate arguments.

Also copy the top-level catalog generated by the publisher into the output
directory.

The JSON input file should be in the format as required by `make_mapfiles.py'.
"""
import os
import sys
import argparse
import json
import shutil

import psycopg2

from esacci_esgf.modify_catalogs import ProcessBatch
from esacci_esgf.input.parse_esg_ini import EsgIniParser


class CatalogGetter(object):
    def __init__(self, esg_ini, output_dir=None, ncml_dir=None,
                 remote_agg_dir=None):
        self.output_dir = output_dir
        self.ncml_dir = ncml_dir
        self.remote_agg_dir = remote_agg_dir

        # Parse esg.ini config file
        self.dburl = EsgIniParser.get_value(esg_ini, "publication_db_url")
        # This is the directory that paths in the DB are relative to
        self.thredds_root = EsgIniParser.get_value(esg_ini, "thredds_root")
        # Directory in which data is stored -- required to translate thredds
        # dataset roots to real paths
        self.data_dir = EsgIniParser.get_value(esg_ini, "thredds_data_path")
        # The hostname of the THREDDS server that will host the data. Required
        # to link to THREDDS catalogues within global attributes in
        # aggregations
        self.thredds_host = EsgIniParser.get_value(esg_ini, "thredds_host")

    def get_catalog_locations(self, ds_names):
        """
        Return a dictionary mapping dataset name to path of the corresponding
        THREDDS catalog produced by the ESGF publisher
        """
        conn = psycopg2.connect(self.dburl)
        cursor = conn.cursor()
        cursor.execute("SELECT dataset_name, version, location FROM catalog;")

        locations = {}

        for name, version, location in cursor:
            # Name from JSON file contains version whereas name in DB does not,
            # since several versions of a dataset may exist. Thus we lookup the
            # concatenated name and version
            versioned_ds_name = "{}.v{}".format(name, version)

            if versioned_ds_name in ds_names:
                locations[versioned_ds_name] = location

        return locations

    def get_and_modify(self, json_filename):
        """
        Parse a JSON file to get dataset names, retrieve the associated
        catalogs and modify them as necessary
        """
        with open(json_filename) as f:
            json_doc = json.load(f)

        ds_names = json_doc.keys()
        cat_locations = self.get_catalog_locations(ds_names)

        # Print a warning if not all datasets in JSON were found in the DB
        not_found = set(ds_names) - set(cat_locations.keys())
        if not_found:
            print("WARNING: Failed to find the following datasets in the DB:",
                  file=sys.stderr)
            for ds_name in sorted(not_found):
                print(ds_name, file=sys.stderr)
            print("", file=sys.stderr)

        for ds_name, cat_loc in cat_locations.items():
            info = json_doc[ds_name]
            # Need to preserve the directory structure found under
            # thredds catalog root so that the links in the top-level catalog
            # are correct when catalogs are moved.
            #
            # Thus take the directory name from the cat_loc and append it to
            # output dir
            output_dir = os.path.join(self.output_dir, os.path.dirname(cat_loc))
            if not os.path.isdir(output_dir):
                os.mkdir(output_dir)

            options = ["--output-dir", output_dir, "--ncml-dir", self.ncml_dir,
                       "--remote-agg-dir", self.remote_agg_dir, "--data-dir",
                       self.data_dir, "--server", self.thredds_host]
            if info["generate_aggregation"]:
                options.append("--aggregate")

                if info["include_in_wms"]:
                    options.append("--wms")

            options.append(os.path.join(self.thredds_root, cat_loc))
            pb = ProcessBatch(options)
            pb.do_all()

    def copy_top_level_catalog(self):
        """
        Copy the top level catalog generated by the publisher to the output
        directory
        """
        basename = "catalog.xml"
        src = os.path.join(self.thredds_root, basename)
        dest = os.path.join(self.output_dir, basename)
        shutil.copyfile(src, dest)


def main():
    parser = argparse.ArgumentParser(
        description=__doc__,
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    parser.add_argument(
        "input_json",
        nargs="*",
        help="JSON file(s) containing IDs of datasets to obtain the modified "
             "catalog for"
    )
    parser.add_argument(
        "-e", "--esg-ini",
        required=True,
        help="Path to esg.ini containing DB connection URL and THREDDS "
             "catalog root directory"
    )
    # These arguments are passed straight through to modify_catalogs
    parser.add_argument(
        "-o", "--output-dir",
        dest="output_dir",
        required=True,
        help="Directory to write modified catalog(s) and top-level catalog to"
    )
    parser.add_argument(
        "-n", "--ncml-dir",
        dest="ncml_dir",
        required=True,
        help="Directory to write NcML aggregations to"
    )
    parser.add_argument(
        "--remote-agg-dir",
        dest="remote_agg_dir",
        required=True,
        help="Directory under which NcML aggregations are stored on the "
             "TDS server"
    )

    args = parser.parse_args(sys.argv[1:])
    getter = CatalogGetter(args.esg_ini, args.output_dir, args.ncml_dir,
                           args.remote_agg_dir)
    for json_filename in args.input_json:
        getter.get_and_modify(json_filename)
    getter.copy_top_level_catalog()
